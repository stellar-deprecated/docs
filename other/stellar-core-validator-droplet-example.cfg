###########################
## Example Settings for a Micro Cloud Instance Validator
# see https://www.stellar.org/developers/stellar-core/learn/admin.html#validating
# it is kept up to date based on https://github.com/stellar/docs/blob/master/validators.md

# LOG_FILE_PATH (string) default "stellar-core.log"
# Path to the file you want stellar-core to write its log to.
# You can set to "" for no log file.
# Stellar-core also log also found in /var/log/syslog
LOG_FILE_PATH=""

# BUCKET_DIR_PATH (string) default "buckets"
# Specifies the directory where stellar-core should store the bucket list.
# This will get written to a lot and will grow as the size of the ledger grows.
BUCKET_DIR_PATH="/var/lib/stellar/buckets"

# DATABASE (string) default "sqlite3://:memory:"
# Sets the DB connection string for SOCI.
# Defaults to an in memory database.
# If using sqlite, a string like:
#
#   "sqlite3://path/to/dbname.db"
#
# alternatively, if using postgresql, a string like:
#
#   "postgresql://dbname=stellar user=xxxx password=yyyy host=10.0.x.y"
#
# taking any combination of parameters from:
#
#   http://www.postgresql.org/docs/devel/static/libpq-connect.html#LIBPQ-PARAMKEYWORDS
#
DATABASE="postgresql://dbname=stellar user=stellar password=PLACE_PASSWORD_HERE host=localhost"

# HTTP_PORT (integer) default 11626
# What port stellar-core listens for commands on.
HTTP_PORT=11626

# PUBLIC_HTTP_PORT (true or false) default false
# If false you only accept stellar commands from localhost.
# Do not set to true and expose the port to the open internet. This will allow
#  random people to run stellar commands on your server. (such as `stop`)
PUBLIC_HTTP_PORT=false

# Maximum number of simultaneous HTTP clients
HTTP_MAX_CLIENT=128

# COMMANDS  (list of strings) default is empty
# List of commands to run on startup.
# Right now only setting log levels really makes sense.
COMMANDS=[
"ll?level=info&partition=Herder"
]

# convenience mapping of common names to node IDs. The common names can be used
#  in the .cfg. `$common_name`. If set, they will also appear in your logs
#  instead of the less friendly nodeID.
#  VERIFY at https://dashboard.stellar.org  -and-
#  https://github.com/stellar/docs/blob/master/validators.md
#
NODE_NAMES=[
## Stellar Development Foundation (SDF)
"GCGB2S2KGYARPVIA37HYZXVRM2YZUEXA6S33ZU5BUDC6THSB62LZSTYH sdf1",
"GCM6QMP3DLRPTAZW2UZPCPX2LF3SXWXKPMP3GKFZBDSF3QZGV2G5QSTK sdf2",
"GABMKJM6I25XI4K7U6XWMULOUQIQ27BCTMLS6BYYSOWKTBUXVRJSXHYQ sdf3",
## SDF Verified Validators
"GCGWABAQ6OUOVUGWJVPRJ5LWBIWYN3CVOVOZYBNQQGIBRULQHYNGQ7GH cryptomover1",
"GC7MH45NSXXPBLQJRSEVF2DFUVLGGYOJER5FRUNVCYVMXJYJT5LLQJW5 cryptomover2",
"GAXEJOMEVVD5OAAGOOZ4SXTTNR46C4V23XROHWRNVEGOTGOABO3ZVH7Z fairx-us",
"GCKWUQGSVO45ZV3QK7POYL7HMFWDKWJVMFVEGUJKCAEVUITUCTQWFSM6 ibm-au",
"GBUJA3Z5TLAKLI5MEH4TETLXJBQVSVW74MNEKP5UUHTP3IMLNSUPOTVA ibm-br",
"GB2HF2NHRKKFZYFDGD7MUENOYROOEK7SWYV2APYOODP6P7BUJTLILKIL ibm-ca",
"GBJ7T3BTLX2BP3T5Q4256PUF7JMDAB35LLO32QRDYE67TDDMN7H33GGE ibm-hk",
"GCH3O5PTCZVR4G65W3B4XDKWI5V677HQB3QO7CW4YPVYDDFBE2GE7G6V ibm-in",
"GAEEH4TBR7YQQWKJ2FIT57HXZZTMK2BX5LY4POJUYFSEZ7Y2ONHPPTES ibm-it",
"GDRA72H7JWXAXWJKOONQOPH3JKNSH5MQ6BO5K74C3X6FO2G3OG464BPU ibm-no",
"GAENPO2XRTTMAJXDWM3E3GAALNLG4HVMKJ4QF525TR25RI42YPEDULOW ibm-uk",
"GARBCBH4YSHUJLYEPKEPMVYZIJ3ZSQR3QCJ245CWGY64X72JLN4A6RSG ibm-usa",
"GC5SXLNAM3C4NMGK2PXK4R34B5GNZ47FYQ24ZIBFDFOCU6D4KBN4POAE satoshipay1",
"GBJQUIXUO4XSNPAUT6ODLZUJRV2NPXYASKUBY4G5MYP3M47PCVI55MNT satoshipay2",
"GAK6Z5UVGUVSEK6PEOCAYJISTT5EJBB34PN3NOLEQG2SUKXRVV2F6HZY satoshipay3",
"GCJCSMSPIWKKPR7WEPIQG63PDF7JGGEENRC33OKVBSPUDIRL6ZZ5M7OO tempo",
## SDF Unverified Validators
"GDNSWA3ZJRQI7EBFLUASFZCMXU3WDEOP24M373XTADCT7UYGWBNFWZSK alchymia",
"GACEYMZ5RGGPV3N7WCZ2VWNLA3PZGBBGBKXP4G3GQ7FTIV5IR5ZZEB2R badbox",
"GCXEKMPY6TDNNI2VBWFOEA23V22OZW2INSCGTKNEHIHJDPG2C53PURBG chris.network1",
"GDCAQY2Q5VMMI4ZOX35GTL7X43LMP3RA5W4VCP6V2BB6IFOO76GACXZU chris.network2",
"GAOO3LWBC4XF6VWRP5ESJ6IBHAISVJMSBTALHOQM2EZG7Q477UWA6L7U eno",
"GC3WHI444R4ULETZAECXA6B6PI4OHVWUW2RKZYP56H6MFL5IFAXAOYEW flutterwave",
"GAZMF5YXLMTUTMAXRZ3Z3TOJ5F54OZJGRQAUXDY2VZN3JRYD2MIGVH7B fosha",
"GD63J64WLRF3IOLJZODAVLB4PQNMFFLDBIZCRC4GLTLCUE4ZQMIKPJK6 mcna",
"GCJWQ4C5VQKLAVHT2Y6CA7TIM3VBEH62E7LKTIQQJDBZFLMZPFY3ITTV mystellar.tools",
"GAG5HH3VF7WLUR3TRQSJGNX66GSEA5HKH3QPIYREDCT4JXUCD4U43JBJ poiuty",
"GBB32UXWEXGZUE7H7LUVNNZRT3ZMZ3YH7SP3V5EFBILUVL3NCTSSK3IZ stellarport1",
"GC5A5WKAPZU5ASNMLNCAMLW7CVHMLJJAKHSZZHE2KWGAJHZ4EW6TQ7PB stellarport2",
"GBRILQP7RHIEBJPLGZ5XP6YWTYFSOQARAKECACMEMGXR53X6IX75E2L5 stellarport3",
"GDIQKLQVOCD5UD6MUI5D5PTPVX7WTP5TAPP5OBMOLENBBD5KG434KYQ2 stronghold1",
"GD7FVHL2KUTUYNOJFRUUDJPDRO2MAZJ5KP6EBCU6LKXHYGZDUFBNHXQI umbrel"
]

###########################
## Configure which network this instance should talk to
NETWORK_PASSPHRASE="Public Global Stellar Network ; September 2015"

###########################
## Overlay configuration

# Percentage, between 0 and 100, of system activity (measured in terms
# of both event-loop cycles and database time) below-which the system
# will consider itself "loaded" and attempt to shed load. Set this
# number low and the system will be tolerant of overloading. Set it
# high and the system will be intolerant. By default it is 0, meaning
# totally insensitive to overloading.
MINIMUM_IDLE_PERCENT=0

# PEER_PORT (Integer) defaults to 11625
# The port other instances of stellar-core can connect to you on.
PEER_PORT=11625

# TARGET_PEER_CONNECTIONS (Integer) default 8
# This controls how aggressively the server will connect to other peers.
# It will send outbound connection attempts until it is at this
#   number of peer connections.
TARGET_PEER_CONNECTIONS=4

# MAX_ADDITIONAL_PEER_CONNECTIONS (Integer) default -1
# Numbers of peers above the target that can connect to this instance
# This gives room for other peers to connect to this instance.
# Setting this too low will result in peers stranded out of the network
# -1: use TARGET_PEER_CONNECTIONS as value for this field
MAX_ADDITIONAL_PEER_CONNECTIONS=6

# MAX_PENDING_CONNECTIONS (Integer) default 5000
# Maximum number of pending (non authenticated) connections to this server.
# Next connections will be dropped immediately.
MAX_PENDING_CONNECTIONS=256

# PEER_AUTHENTICATION_TIMEOUT (Integer) default 2
# This server will drop peer that does not authenticate itself during that
# time.
PEER_AUTHENTICATION_TIMEOUT=2

# PEER_TIMEOUT (Integer) default 30
# This server will drop peer that does not send or receive anything during that
# time when authenticated.
PEER_TIMEOUT=30

# PREFERRED_PEERS (list of strings) default is empty
# These are IP:port strings that this server will add to its DB of peers.
# This server will try to always stay connected to the other peers on this list.
PREFERRED_PEERS=[
"core-live-a.stellar.org",
"core-live-b.stellar.org",
"core-live-c.stellar.org"
]

# PREFERRED_PEER_KEYS (list of strings) default is empty
# These are public key identities that this server will treat as preferred
# when connecting, similar to the PREFERRED_PEERS list.
# can use a name already defined in the .cfg
PREFERRED_PEER_KEYS=[]

# PREFERRED_PEERS_ONLY (boolean) default is false
# When set, this peer will only connect to PREFERRED_PEERS and will only
# accept connections from PREFERRED_PEERS or PREFERRED_PEER_KEYS
PREFERRED_PEERS_ONLY=false

# KNOWN_PEERS (list of strings) default is empty
# These are IP:port strings that this server will add to its DB of peers.
# It will try to connect to these when it is below TARGET_PEER_CONNECTIONS.
KNOWN_PEERS=[
"au.stellar.ibm.com",
"br.stellar.ibm.com",
"ca.stellar.ibm.com",
"hk.stellar.ibm.com",
"it.stellar.ibm.com",
"in.stellar.ibm.com",
"no.stellar.ibm.com",
"uk.stellar.ibm.com",
"us.stellar.ibm.com",
"ohio-1.stellar.stellarport.io",
"ohio-2.stellar.stellarport.io",
"ohio-3.stellar.stellarport.io",
"stellar.256kw.com",
"stellar.papayame.com",
"stellar.crservers.com",
"stellar1.tempo.eu.com",
"uswest.stellar.fairx.io",
"stellar.chris.network",
"stellar.alchymia.io",
"chicago.badbox.io",
"cryptodealer.hk",
"cryptomover.org",
"mystellar.tools",
"uswest.stellar.fairx.io",
"validator1.stellar.stronghold.co"
]

#######################
##  SCP settings

# NODE_SEED (string) default random, regenerated each run.
# The seed used for generating the public key this node will
# be identified with in SCP.
# Your seed should be unique. Protect your seed. Treat it like a password.
# If you don't set a NODE_SEED one will be generated for you randomly
# on each startup.
# 
# To generate a new, stable seed (and associated public key), run:
#
#  stellar-core --genseed
# 
# You only need to keep the seed from this; you can always recover the
# public key from the seed by running:
#
#  stellar-core --convertid <seed>
#
NODE_SEED="PLACE_SEED_HERE_BEGINS_WITH_G self"

# NODE_IS_VALIDATOR (boolean) default false.
# Only nodes that want to participate in SCP should set NODE_IS_VALIDATOR=true.
# Most instances should operate in observer mode with NODE_IS_VALIDATOR=false.
# See QUORUM_SET below.
NODE_IS_VALIDATOR=true

###########################
# Consensus settings

# FAILURE_SAFETY (integer) default -1
# Most people should leave this to -1
# This is the maximum number of validator failures from your QUORUM_SET that
# you want to be able to tolerate.
# Typically, you will need at least 3f+1 nodes in your quorum set.
# If you don't have enough nodes in your quorum set to tolerate the level you
#  set here stellar-core won't run as a precaution.
# A value of -1 indicates to use (n-1)/3 (n being the number of nodes
#    and groups from the top level of your QUORUM_SET)
# A value of 0 is only allowed if UNSAFE_QUORUM is set
# Note: The value of 1 below is the maximum number derived from the value of
# QUORUM_SET in this configuration file
FAILURE_SAFETY=-1

# UNSAFE_QUORUM (true or false) default false
# Most people should leave this to false.
# If set to true allows to specify a potentially unsafe quorum set.
# Otherwise it won't start if
#   a threshold % is set too low (threshold below 66% for the top level,
#       51% for other levels)
#   FAILURE_SAFETY at 0 or above the number of failures that can occur
# You might want to set this if you are running your own network and
#  aren't concerned with byzantine failures or if you fully understand how the
# quorum sets of other nodes relate to yours when it comes to
# quorum intersection.
UNSAFE_QUORUM=false

#########################
##  History

# CATCHUP_COMPLETE (true or false) defaults to false
# if true will catchup to the network "completely" (replaying all history)
# if false will look for CATCHUP_RECENT for catchup settings
CATCHUP_COMPLETE=false

# CATCHUP_RECENT (integer) default to 0
# if CATCHUP_COMPLETE is true this option is ignored
# if set to 0 will catchup "minimally", using deltas to the most recent
# snapshot.
# if set to any other number, will catchup "minimally" to some past snapshot,
# then will reply history from that point to current snapshot, ensuring that
# at least CATCHUP_RECENT number of ledger entries will be present in database
# if "some past snapshot" is already present in database, it just replays all
# new history
CATCHUP_RECENT=0

# MAX_CONCURRENT_SUBPROCESSES (integer) default 16
# History catchup can potentialy spawn a bunch of sub-processes.
# This limits the number that will be active at a time.
MAX_CONCURRENT_SUBPROCESSES=10

# FUTURE SETTING
# AUTOMATIC_MAINTENANCE_PERIOD (integer, seconds) default 3600
# Interval between automatic maintenance executions
# Set to 0 to disable automatic maintenance
#AUTOMATIC_MAINTENANCE_PERIOD=3600

# FUTURE SETTING
# AUTOMATIC_MAINTENANCE_COUNT (integer) default 50000
# Number of unneeded rows in each table that will be removed during one
# maintenance run.
# Set to 0 to disable automatic maintenance
#AUTOMATIC_MAINTENANCE_COUNT=5000

#####################
##  Tables must come at the end. (TOML you are almost perfect!)

# HISTORY
# Used to specify where to fetch and store the history archives.
# Fetching and storing history is kept as general as possible.
# Any place you can save and load static files from should be usable by the
#  stellar-core history system.   s3, the file system, http, etc
# stellar-core will call any external process you specify and will pass it the
#  name of the file to save or load.
# Simply use template parameters `{0}` and `{1}` in place of the files being transmitted or retrieved.
# You can specify multiple places to store and fetch from. stellar-core will
# use multiple fetching locations as backup in case there is a failure fetching from one.
# 
# Note: any archive you *put* to you must run `$ stellar-core --newhist <historyarchive>`
#       once before you start.
#       for example this config you would run: $ stellar-core --newhist local
[HISTORY.local]
get="cp /var/lib/stellar/history/vs/{0} {1}"
put="cp {0} /var/lib/stellar/history/vs/{1}"
mkdir="mkdir -p /var/lib/stellar/history/vs/{0}"

# Stellar.org history store
[HISTORY.sdf1]
get="curl -sf http://history.stellar.org/prd/core-live/core_live_001/{0} -o {1}"

[HISTORY.sdf2]
get="curl -sf http://history.stellar.org/prd/core-live/core_live_002/{0} -o {1}"

[HISTORY.sdf3]
get="curl -sf http://history.stellar.org/prd/core-live/core_live_003/{0} -o {1}"


# QUORUM_SET is a required field
# This is how you specify this server's quorum set.
# 
# It can be nested up to 2 levels: {A,B,C,{D,E,F},{G,H,{I,J,K,L}}}
# THRESHOLD_PERCENT is how many have to agree (1-100%) within a given set.
# Each set is treated as one vote.
# So for example in the above there are 5 things that can vote:
# individual validators: A,B,C, and the sets {D,E,F} and {G,H with subset {I,J,K,L}}
# the sets each have their own threshold.
# For example with {100% G,H with subset (50% I,J,K,L}}
#   means that quorum will be met with G, H and any 2 (50%) of {I, J, K, L}
# 
# a [QUORUM_SET.path] section is constructed as
# THRESHOLD_PERCENT: how many have to agree, defaults to 67 (rounds up).
# VALIDATORS: array of node IDs
# additional subsets [QUORUM_SET.path.item_number]
# a QUORUM_SET
#  must not contain duplicate entries {{A,B},{A,C}} is invalid for example
#  The key for "self" is implicitely added at the top level, so the effective
#   quorum set is [t:2, self, QUORUM_SET]. Note that "self" is always agreeing
#   with the instance (if QUORUM_SET includes it)
#       
# The following setup is equivalent to the example given above.
#
# Note on naming: you can add common names to the NAMED_NODES list here as
#  shown in the first 3 validators or use common names that have been
#  previously defined.
[QUORUM_SET]
VALIDATORS=[
"$sdf1","$sdf2","$sdf3",
"$cryptomover2",
"$ibm-au","$ibm-it",
"$satoshipay1","$satoshipay2","$satoshipay3",
"$stronghold1"
]

[QUORUM_SET.trial]
THRESHOLD_PERCENT=51
VALIDATORS=[
"$chris.network1",
"$chris.network2",
"$badbox",
"$eno",
"$fosha",
"$stellarport1",
"$umbrel"
]
